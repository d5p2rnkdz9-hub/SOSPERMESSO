---
phase: 18-seo-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - robots.txt
  - sitemap.xml
  - scripts/build-sitemap.js
  - package.json
autonomous: true

must_haves:
  truths:
    - "Googlebot can access all pages (robots.txt allows crawling)"
    - "Search engines can find sitemap.xml via robots.txt Sitemap directive"
    - "Sitemap lists all non-redirect HTML pages with lastmod dates"
    - "Running npm run build:sitemap regenerates sitemap from file system"
    - "Redirect pages (meta refresh) are excluded from sitemap"
  artifacts:
    - path: "robots.txt"
      provides: "Crawler access rules and sitemap reference"
      contains: "User-agent"
    - path: "sitemap.xml"
      provides: "XML sitemap with all indexable pages"
      contains: "<urlset"
    - path: "scripts/build-sitemap.js"
      provides: "Sitemap generation script"
      exports: ["build function"]
    - path: "package.json"
      provides: "npm script for sitemap generation"
      contains: "build:sitemap"
  key_links:
    - from: "robots.txt"
      to: "sitemap.xml"
      via: "Sitemap directive"
      pattern: "Sitemap:.*sitemap\\.xml"
    - from: "scripts/build-sitemap.js"
      to: "sitemap.xml"
      via: "file write"
      pattern: "writeFile.*sitemap\\.xml"
---

<objective>
Create SEO infrastructure enabling search engine discovery and crawling of all site pages.

Purpose: Without robots.txt and sitemap.xml, search engines must discover pages through crawling links, which is slower and may miss orphan pages. This phase provides explicit crawler guidance and a complete page inventory.

Output: robots.txt (static), sitemap.xml (generated), build-sitemap.js (generator), npm script
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/18-seo-infrastructure/18-RESEARCH.md
@scripts/build-documents.js
@package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create robots.txt</name>
  <files>robots.txt</files>
  <action>
Create robots.txt in the site root with the following content:

```
# robots.txt for sospermesso.it
# Allow all search engines to crawl all content

User-agent: *
Allow: /

Sitemap: https://sospermesso.it/sitemap.xml
```

This is a static file - no build process needed. The file allows all crawlers and points to the sitemap.
  </action>
  <verify>
- File exists at project root: `ls robots.txt`
- Contains User-agent and Sitemap directives: `cat robots.txt`
- Sitemap URL is absolute and correct: grep for `https://sospermesso.it/sitemap.xml`
  </verify>
  <done>robots.txt exists at site root with Allow directive and Sitemap reference</done>
</task>

<task type="auto">
  <name>Task 2: Create sitemap build script and generate sitemap.xml</name>
  <files>scripts/build-sitemap.js, sitemap.xml</files>
  <action>
1. Install the xml npm package:
   ```bash
   npm install xml
   ```

2. Create `scripts/build-sitemap.js` following the patterns from build-documents.js:

**Script requirements:**
- Use `fs/promises` for async file operations (like existing scripts)
- Use `xml` package for XML generation (handles escaping automatically)
- Site URL: `https://sospermesso.it`

**Files to include in sitemap:**
- `index.html` in root
- All `*.html` files in `src/pages/` (including subdirectories like `permesso-lavoro-subordinato/`)

**Files to EXCLUDE:**
- `404.html` (error pages should not be indexed)
- `NOTION_WEBSITE/` directory (export artifacts)
- Any file containing `http-equiv="refresh"` in first 500 chars (redirect pages)
- `node_modules/`, `.git/`, `.planning/` directories

**Redirect detection:**
Read the first 500 characters of each HTML file. If it contains `http-equiv="refresh"` or `http-equiv='refresh'`, exclude from sitemap.

**Lastmod format:**
Use file modification time: `stats.mtime.toISOString().split('T')[0]` (YYYY-MM-DD)

**URL construction:**
- Root files: `https://sospermesso.it/index.html`
- Subpages: `https://sospermesso.it/src/pages/chi-siamo.html`
- Nested: `https://sospermesso.it/src/pages/permesso-lavoro-subordinato/index.html`

**Output:**
Write sitemap.xml to project root with XML declaration and proper urlset namespace.

**Console output (following build-documents.js pattern):**
- Show script header with emoji
- Log count of pages found
- Log count of redirects excluded
- Show output file path

3. Run the script to generate initial sitemap.xml:
   ```bash
   node scripts/build-sitemap.js
   ```
  </action>
  <verify>
- Script exists: `ls scripts/build-sitemap.js`
- Script runs without error: `node scripts/build-sitemap.js`
- sitemap.xml exists: `ls sitemap.xml`
- sitemap.xml is valid XML: `head -20 sitemap.xml` shows proper XML structure
- sitemap.xml contains expected URLs: `grep "sospermesso.it" sitemap.xml | head -5`
- Redirect pages excluded: `grep "http-equiv" sitemap.xml` should return nothing
- Count is reasonable: `grep -c "<url>" sitemap.xml` should be ~220-260 (262 total minus ~38 redirects minus 404)
  </verify>
  <done>build-sitemap.js generates sitemap.xml listing all non-redirect HTML pages with lastmod dates</done>
</task>

<task type="auto">
  <name>Task 3: Add npm script for sitemap generation</name>
  <files>package.json</files>
  <action>
Add `build:sitemap` script to package.json scripts section:

```json
"scripts": {
  "test": "echo \"Error: no test specified\" && exit 1",
  "build:docs": "node scripts/build-documents.js",
  "build:sitemap": "node scripts/build-sitemap.js"
}
```

This follows the existing `build:docs` naming convention.
  </action>
  <verify>
- package.json contains build:sitemap: `grep "build:sitemap" package.json`
- Script runs via npm: `npm run build:sitemap`
- Script completes successfully with exit code 0
  </verify>
  <done>`npm run build:sitemap` regenerates sitemap.xml from current files</done>
</task>

</tasks>

<verification>
After all tasks complete, verify the full SEO infrastructure:

1. **robots.txt accessibility:**
   ```bash
   cat robots.txt
   ```
   Should show User-agent, Allow, and Sitemap directives.

2. **sitemap.xml validity:**
   ```bash
   head -30 sitemap.xml
   ```
   Should show valid XML with urlset and url elements.

3. **Redirect exclusion:**
   ```bash
   # Pick a known redirect file and verify it's not in sitemap
   grep "permesso-asilo.html" sitemap.xml
   ```
   Should return nothing (permesso-asilo.html is a redirect).

4. **Regeneration works:**
   ```bash
   rm sitemap.xml
   npm run build:sitemap
   ls sitemap.xml
   ```
   sitemap.xml should be recreated.

5. **Page count sanity check:**
   ```bash
   # Count URLs in sitemap
   grep -c "<url>" sitemap.xml
   # Should be approximately 220-230 pages (262 total - ~38 redirects - 404)
   ```
</verification>

<success_criteria>
- [ ] robots.txt exists at site root with User-agent: *, Allow: /, and Sitemap directive
- [ ] sitemap.xml exists at site root with valid XML structure
- [ ] sitemap.xml contains only non-redirect HTML pages (no meta refresh pages)
- [ ] sitemap.xml includes lastmod dates in YYYY-MM-DD format
- [ ] `npm run build:sitemap` command works and regenerates sitemap
- [ ] Sitemap URL count is ~220-230 (reasonable after excluding redirects and 404)
</success_criteria>

<output>
After completion, create `.planning/phases/18-seo-infrastructure/18-01-SUMMARY.md`
</output>
